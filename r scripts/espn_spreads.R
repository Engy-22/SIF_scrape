# load libraries and set working directory to repo
library(rvest)
library(tidyr)
library(stringr)
setwd("~/G_WD/SIF_scrape")

# run phantomjs javascript file to render javascript variables and save as html in working directory
print("running phantomjs...")

system("./phantomjs scrape_espn.js")

print("COMPLETE")

# read html file generated by phantomjs
html = read_html('espn.html')

# scrape week
week_raw = "Week 1, 2017"

# scrape teams 
print("scraping teams...")

teams_raw = html %>%
    html_nodes(".sb-team-short") %>%
    html_text()
teams = as.data.frame(cbind(away = teams_raw[c(TRUE,FALSE)], 
                            home = teams_raw[c(FALSE,TRUE)]), 
                      stringsAsFactors = FALSE)
print("COMPLETE")


# scrape team short names
print("scraping team short names...")

shorts_raw = html %>%
    html_nodes(".sb-team-abbrev") %>%
    html_text()
shorts = as.data.frame(cbind(away_short = shorts_raw[c(TRUE,FALSE)], 
                             home_short = shorts_raw[c(FALSE,TRUE)]),
                       stringsAsFactors = FALSE)

print("COMPLETE")        


# scrape espn game ids
print("scraping espn game ids...")

egame_id = html %>%
    html_nodes(".scoreboard") %>%
    html_attr("id")
egame_id = na.omit(egame_id)
egame_id = as.integer(egame_id)

print("COMPLETE")        


# scrape date-times
print("scraping date times...")

dates = html %>%
    html_nodes(".date-time") %>%
    html_attr("data-date") 
dates = gsub("T", " ", dates)
dates = gsub("Z", "", dates)
dates = as.POSIXlt(dates)
dates = as.data.frame(str_split_fixed(dates, " ", 2),
                      stringsAsFactors = FALSE)
colnames(dates) = c("date", "time")

print("COMPLETE")        


# bind together into games table
print("binding games table...")

games = as.data.frame(cbind(egame_id, 
                            teams, 
                            shorts,
                            date = dates$date), 
                      stringsAsFactors = FALSE)

print("COMPLETE")        


# loop for espn game ids, times, and lines
print("scrape loop for times and lines...")

lines = character(0)
times = character(0)

for(i in egame_id){
    line = html %>%
        html_nodes(paste("#", i,  " .line", sep = "")) %>%
        html_text()
    line = paste(i, line, sep = " ")
    
    time = html %>%
        html_nodes(paste("#", i,  " .time", sep = "")) %>%
        html_text()
    time = paste(i, time, sep = " ")
    
    lines = append(lines, line)
    times = append(times, time)
}

lines = as.data.frame(str_split_fixed(lines, " ", 3),
                      stringsAsFactors = FALSE)
colnames(lines) = c("egame_id", "favorite", "espn_spread")
lines$espn_spread = abs(as.numeric(lines$espn_spread))

times = as.data.frame(str_split_fixed(times, " ", 2),
                      stringsAsFactors = FALSE)
colnames(times) = c("egame_id", "time")
times$time = gsub("ET", "", times$time)

linetime = merge(times, lines, by = "egame_id")

print("COMPLETE")        


# merge all into spreads table
print("merging all...")

spreads = merge(games, linetime, by = "egame_id", all.x = TRUE)

print("COMPLETE")        


# merge game_ids and add timestamp column
map = read.csv("./maps/map1.csv", header = TRUE, stringsAsFactors = FALSE)[,1:2]
spreads = merge(map, spreads, by = "egame_id")
spreads = spreads[order(spreads$game_id),]
spreads$update_ts = Sys.time()

# output to .csv
print("writing csv...")

write.csv(spreads, 
          paste("./output/", week_raw, "/", Sys.Date(),"/espn_", Sys.Date() ,".csv", sep = ""), 
          row.names = FALSE)

print("COMPLETE")